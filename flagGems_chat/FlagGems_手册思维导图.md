# FlagGems 手册思维导图

## 快速入门指南
### 安装指南
- **安装构建依赖**：安装必要的构建工具
  - 何时使用：在安装 FlagGems 之前
  - 命令：`pip install -U scikit-build-core>=0.11 pybind11 ninja cmake`
  - **FAQ**：
    - Q: 为什么需要安装这些构建依赖？
      A: 这些依赖是构建 FlagGems 所必需的，特别是当你需要构建 C 扩展以获得更好性能时。
    - Q: 如果我不安装这些依赖会怎样？
      A: 你仍然可以安装 FlagGems 作为纯 Python 包，但无法使用 C++ 运行时优化。
    - Q: 这些依赖的版本要求严格吗？
      A: 是的，特别是 scikit-build-core 需要 0.11 或更高版本，否则可能会导致构建失败。
    - Q: 如何检查这些依赖是否已正确安装？
      A: 可以使用 `pip list` 命令查看已安装的包及其版本，确保所有依赖都已安装且版本符合要求。

- **安装步骤**：
  1. 克隆仓库：`git clone https://github.com/flagos-ai/FlagGems.git`
  2. 安装依赖：`pip install -r flag_tree_requirements/requirements_nvidia.txt`
  3. 安装 FlagGems：`pip install --no-build-isolation .`
  - **FAQ**：
    - Q: 我需要使用哪个后端的依赖文件？
      A: 取决于你的硬件平台，例如 NVIDIA 平台使用 `requirements_nvidia.txt`，其他平台使用对应的后端依赖文件。
    - Q: 什么是 `--no-build-isolation` 参数？
      A: 这个参数告诉 pip 在当前环境中构建包，而不是创建一个隔离的构建环境，这样可以确保使用你已经安装的依赖。
    - Q: 如果我想使用最新的开发版本，应该如何安装？
      A: 可以使用 `pip install --no-build-isolation -e .` 进行可编辑安装，这样当你修改代码时，不需要重新安装。
    - Q: 安装过程中遇到编译错误怎么办？
      A: 首先检查构建依赖是否正确安装且版本符合要求，然后检查你的编译环境是否完整，例如是否安装了 C++ 编译器。
    - Q: 如何验证 FlagGems 是否成功安装？
      A: 可以尝试导入 `import flag_gems`，如果没有报错，则说明安装成功。

### 使用方法
- **全局启用**：在整个应用中启用 FlagGems
  - 何时使用：当你希望整个应用都使用 FlagGems 加速时
  - 代码：`flag_gems.enable()`
  - **FAQ**：
    - Q: 全局启用会影响所有算子吗？
      A: 不会，只会影响 FlagGems 支持的算子，其他算子仍然使用 PyTorch 的原生实现。
    - Q: 全局启用后如何禁用？
      A: 一旦全局启用，当前进程中无法完全禁用，但你可以使用 `unused` 参数排除特定算子。
    - Q: 全局启用的最佳实践是什么？
      A: 建议在应用程序的最开始就启用 FlagGems，这样所有后续的计算都能受益于加速。
    - Q: 全局启用会影响 PyTorch 的其他功能吗？
      A: 不会，FlagGems 只是替换了部分算子的实现，不会影响 PyTorch 的其他功能。
    - Q: 在 Jupyter Notebook 中使用时，全局启用会持续多久？
      A: 在 Jupyter Notebook 中，全局启用会持续到内核重启或关闭。

- **上下文管理器**：在特定代码块中临时启用
  - 何时使用：当你只希望部分代码使用 FlagGems 加速时（如基准测试）
  - 代码：`with flag_gems.use_gems():`
  - **FAQ**：
    - Q: 上下文管理器适合什么场景？
      A: 适合基准测试（比较使用和不使用 FlagGems 的性能差异）、调试（隔离问题）等场景。
    - Q: 上下文管理器内部的设置会影响外部吗？
      A: 不会，上下文管理器的作用域仅限于其内部代码块。
    - Q: 上下文管理器可以嵌套使用吗？
      A: 可以，嵌套使用时，内层的设置会覆盖外层的设置，但只在内层作用域内有效。
    - Q: 在上下文管理器中使用 include 或 exclude 参数时，需要注意什么？
      A: 这些参数只在当前上下文管理器内有效，不会影响其他代码块或全局设置。
    - Q: 上下文管理器的性能开销大吗？
      A: 不大，上下文管理器只是临时修改了算子的分发机制，开销很小。

- **显式调用**：直接调用 FlagGems 算子
  - 何时使用：当你需要精确控制哪些算子使用 FlagGems 时
  - 代码：`from flag_gems import ops; ops.mm(a, b)`
  - **FAQ**：
    - Q: 显式调用和其他启用方式有什么区别？
      A: 显式调用绕过了 PyTorch 的分发机制，直接调用 FlagGems 实现，无需先启用 FlagGems。
    - Q: 什么时候应该使用显式调用？
      A: 当你需要精确控制哪些算子使用 FlagGems，或者在某些特殊场景下 PyTorch 分发机制不工作时。
    - Q: 显式调用的性能与其他启用方式相比如何？
      A: 性能基本相同，因为最终都是调用相同的 FlagGems 实现，只是调用路径不同。
    - Q: 显式调用支持所有 FlagGems 算子吗？
      A: 是的，所有 FlagGems 支持的算子都可以通过 `flag_gems.ops` 模块显式调用。
    - Q: 显式调用时需要注意什么？
      A: 需要确保输入参数的类型和形状符合算子的要求，否则可能会导致错误。

## 高级用法
### 基本用法选项
- **全局启用**：在整个脚本中应用 FlagGems 优化
  - **FAQ**：
    - Q: 全局启用会增加内存使用吗？
      A: 可能会有轻微增加，因为 FlagGems 需要加载额外的内核和缓存，但通常影响很小。
    - Q: 全局启用后如何查看哪些算子被注册了？
      A: 可以使用 `flag_gems.all_registered_ops()` 查看已注册的算子。
    - Q: 全局启用的最佳时机是什么？
      A: 建议在脚本的最开始就启用，这样所有后续的计算都能受益于加速。
    - Q: 全局启用后，如何确认 FlagGems 是否真的在工作？
      A: 可以启用调试日志（`flag_gems.enable(record=True)`），查看哪些算子被 FlagGems 加速了。
    - Q: 全局启用会影响 PyTorch 的其他功能吗？
      A: 不会，FlagGems 只是替换了部分算子的实现，不会影响 PyTorch 的其他功能。

- **选择性启用**：仅启用特定算子
  - 何时使用：当你只想加速一部分操作时
  - 代码：`flag_gems.only_enable(include=["rms_norm", "softmax"])`
  - **FAQ**：
    - Q: 为什么要选择性启用？
      A: 当你发现某些算子在你的工作负载中表现不佳，或者你只想优化模型中的关键部分时。
    - Q: 如何确定哪些算子对我的模型最重要？
      A: 可以使用性能分析工具（如 PyTorch Profiler）来识别模型中的瓶颈算子。
    - Q: 选择性启用比全局启用更高效吗？
      A: 不一定，这取决于你的具体工作负载。如果只有少数算子需要加速，选择性启用可能会稍微减少内存使用，但性能差异通常很小。
    - Q: 如何获取所有可用的算子名称列表？
      A: 可以使用 `flag_gems.all_registered_ops()` 查看所有可用的算子名称。
    - Q: 选择性启用后，如何添加更多算子？
      A: 需要重新调用 `flag_gems.only_enable()` 并包含新的算子列表，因为每次调用都会覆盖之前的设置。

- **作用域启用**：在特定代码块中启用
  - 何时使用：当你需要更精细的控制时
  - 代码：`with flag_gems.use_gems(include=["sum", "add"]):`
  - **FAQ**：
    - Q: 作用域启用中可以同时使用 include 和 exclude 吗？
      A: 可以，但 include 的优先级高于 exclude，如果同时提供，exclude 会被忽略。
    - Q: 作用域启用适合什么场景？
      A: 适合只在模型的特定部分（如注意力机制）使用 FlagGems 加速的场景。
    - Q: 作用域启用可以与全局启用结合使用吗？
      A: 可以，作用域启用的设置会临时覆盖全局设置，但不会永久修改全局设置。
    - Q: 作用域启用中的设置会影响其他线程吗？
      A: 不会，作用域启用的设置只在当前线程的当前代码块中有效。
    - Q: 如何在嵌套的作用域中使用不同的设置？
      A: 可以在内部作用域中使用不同的 include 或 exclude 参数，内部设置会覆盖外部设置，但只在内部作用域内有效。

### 参数概览
- **unused**：禁用特定算子（用于 enable）
  - **FAQ**：
    - Q: 如何确定哪些算子应该添加到 unused 列表中？
      A: 当你发现某个算子在你的工作负载中表现不如预期，或者导致错误时，可以将其添加到 unused 列表中。
    - Q: unused 参数的优先级如何？
      A: unused 参数会覆盖默认的启用行为，确保指定的算子使用 PyTorch 的原生实现。
    - Q: unused 列表中的算子名称必须完全匹配吗？
      A: 是的，必须使用 FlagGems 内部使用的算子名称，你可以通过 `flag_gems.all_registered_ops()` 查看所有可用的算子名称。
    - Q: 如果 unused 列表中的算子不存在会怎样？
      A: 不会报错，只是该算子不会被禁用。
    - Q: 如何查看当前已禁用的算子列表？
      A: 目前没有直接的 API 来查看已禁用的算子列表，但你可以通过检查代码中的 `unused` 参数设置来了解。

- **include**：仅启用特定算子（用于 only_enable）
  - **FAQ**：
    - Q: include 列表中的算子名称必须完全匹配吗？
      A: 是的，必须使用 FlagGems 内部使用的算子名称，你可以通过 `flag_gems.all_registered_ops()` 查看所有可用的算子名称。
    - Q: 如果 include 列表中的算子不存在会怎样？
      A: 不会报错，只是该算子不会被启用。
    - Q: include 参数可以与其他参数结合使用吗？
      A: 是的，include 参数可以与 record、path 等参数结合使用，以实现更精细的控制。
    - Q: 如何确定哪些算子适合添加到 include 列表中？
      A: 可以使用性能分析工具（如 PyTorch Profiler）来识别模型中的瓶颈算子，然后将这些算子添加到 include 列表中。
    - Q: include 列表的顺序重要吗？
      A: 不重要，include 列表的顺序不会影响算子的启用效果。

- **record**：记录算子调用以进行调试或分析
  - **FAQ**：
    - Q: 启用 record 会影响性能吗？
      A: 会有轻微影响，因为需要记录每个算子调用，所以建议只在调试或分析时使用。
    - Q: 记录的日志包含哪些信息？
      A: 包含算子名称、调用时间等信息，帮助你了解哪些算子被 FlagGems 加速了。
    - Q: 日志文件的大小会无限增长吗？
      A: 是的，每次运行都会追加日志，所以建议在不需要时禁用 record，或者定期清理日志文件。
    - Q: 如何分析日志文件中的信息？
      A: 可以使用文本编辑器查看，或者编写简单的脚本分析日志中的算子调用频率和时间分布。
    - Q: 记录的日志可以用于性能分析吗？
      A: 可以，但日志中的时间信息只是近似值，更精确的性能分析建议使用专门的性能分析工具。

- **path**：日志文件路径（仅在 record=True 时使用）
  - **FAQ**：
    - Q: 如果不指定 path，日志会保存在哪里？
      A: 如果不指定 path，默认会在当前目录创建一个日志文件。
    - Q: 日志文件会不断增长吗？
      A: 是的，每次运行都会追加日志，所以建议在不需要时禁用 record，或者定期清理日志文件。
    - Q: 可以指定绝对路径或相对路径吗？
      A: 是的，两种路径格式都支持。
    - Q: 如果指定的目录不存在会怎样？
      A: 会尝试创建目录，如果创建失败，会在当前目录创建日志文件。
    - Q: 可以在不同的运行中使用同一个日志文件吗？
      A: 可以，新的日志会追加到文件末尾，但建议为不同的测试使用不同的日志文件，以便于分析。

### 示例
- **选择性禁用**：`flag_gems.enable(unused=["sum", "add"])`
  - **FAQ**：
    - Q: 为什么我需要选择性禁用某些算子？
      A: 当某些算子在你的工作负载中表现不佳，或者与你的代码有兼容性问题时。
    - Q: 禁用后如何重新启用？
      A: 你需要重新调用 `flag_gems.enable()` 而不包含这些算子在 unused 列表中。
    - Q: 选择性禁用会影响其他算子的性能吗？
      A: 不会，选择性禁用只会影响指定的算子，其他算子的性能不受影响。
    - Q: 可以在运行时动态修改 unused 列表吗？
      A: 不可以，一旦调用 `flag_gems.enable()`，当前进程中的设置就固定了，需要重新调用才能修改。
    - Q: 选择性禁用后，如何验证算子是否真的被禁用了？
      A: 可以启用调试日志，查看算子调用是否使用了 PyTorch 的原生实现。

- **启用调试日志**：`flag_gems.enable(record=True, path="./gems_debug.log")`
  - **FAQ**：
    - Q: 调试日志有什么用？
      A: 可以帮助你确认哪些算子被 FlagGems 加速了，以及它们的调用频率，有助于性能分析。
    - Q: 如何分析调试日志？
      A: 可以查看日志中的算子调用记录，分析哪些算子被频繁调用，以及是否有异常情况。
    - Q: 调试日志的性能影响有多大？
      A: 影响很小，但在高吞吐量场景下可能会有轻微影响，所以建议只在调试或分析时使用。
    - Q: 可以同时启用多个日志文件吗？
      A: 不可以，每次调用 `flag_gems.enable()` 只能指定一个日志文件路径。
    - Q: 调试日志中的时间戳格式是什么？
      A: 调试日志中的时间戳通常是相对时间，从程序启动开始计算，以秒为单位。

- **查询已注册的算子**：`flag_gems.all_registered_ops()`
  - **FAQ**：
    - Q: 为什么我需要查看已注册的算子？
      A: 可以帮助你了解 FlagGems 支持哪些算子，以及它们的名称，这对于选择性启用或禁用算子很有用。
    - Q: 返回的算子名称格式是什么？
      A: 返回的是一个字符串列表，包含所有已注册的算子名称。
    - Q: 已注册的算子列表会随着 FlagGems 版本更新而变化吗？
      A: 是的，随着 FlagGems 版本的更新，支持的算子列表可能会增加或修改。
    - Q: 如何确定某个特定的算子是否被支持？
      A: 可以检查 `flag_gems.all_registered_ops()` 返回的列表中是否包含该算子名称。
    - Q: 已注册的算子是否都经过性能优化？
      A: 不一定，虽然大部分算子都经过了优化，但有些算子可能仍在开发中，性能可能不如预期。

## 多平台支持
- **统一使用接口**：无论底层硬件如何，使用方式完全相同
  - 何时使用：当你需要在不同硬件平台间切换时
  - **FAQ**：
    - Q: 统一接口意味着我可以在不同平台间无缝切换吗？
      A: 是的，代码不需要修改，FlagGems 会自动检测平台并选择合适的后端。
    - Q: 所有平台的性能表现都一样吗？
      A: 不一定，不同平台的硬件特性不同，性能表现可能会有所差异，但 FlagGems 会针对每个平台进行优化。
    - Q: 统一接口是否意味着所有算子在所有平台上都可用？
      A: 不一定，某些算子可能只在特定平台上可用，具体取决于平台的硬件特性和后端实现。
    - Q: 在不同平台间切换时，需要重新安装 FlagGems 吗？
      A: 可能需要，因为不同平台可能需要不同的后端依赖，建议在新平台上重新安装并配置。
    - Q: 统一接口如何处理平台特有的功能？
      A: 统一接口会抽象掉平台差异，提供一致的 API，但在底层会根据平台特性选择最优实现。

- **后端要求**：需要对应的 PyTorch 和 Triton 编译器
  - 如何获取：向硬件供应商请求或使用 FlagTree 项目
  - **FAQ**：
    - Q: 什么是 FlagTree 项目？
      A: FlagTree 是一个统一的 Triton 编译器项目，支持多种 AI 芯片，包括 NVIDIA 和非 NVIDIA 平台。
    - Q: 如何确认我的平台是否支持？
      A: 可以查看 [Supported Platforms](./docs/features.md#platforms-supported) 文档，或者尝试安装并运行 FlagGems。
    - Q: 不同平台的 PyTorch 版本要求是否相同？
      A: 可能不同，建议根据平台要求安装对应的 PyTorch 版本，通常需要较新版本的 PyTorch。
    - Q: FlagTree 与原生 Triton 有什么区别？
      A: FlagTree 是基于原生 Triton 的扩展，添加了对更多硬件平台的支持和优化。
    - Q: 如果没有对应的 Triton 编译器，是否可以使用 FlagGems？
      A: 不能，Triton 编译器是 FlagGems 的核心依赖，没有它无法运行。

- **后端自动检测和手动设置**：
  - 自动检测：默认自动选择后端
  - 手动设置：通过环境变量 `GEMS_VENDOR` 设置
  - 何时使用：当自动检测失败时
  - **FAQ**：
    - Q: 自动检测失败的情况常见吗？
      A: 不常见，大多数情况下自动检测都能正常工作，但在某些特殊环境或新平台上可能需要手动设置。
    - Q: 如何查看当前使用的后端？
      A: 可以使用 `print(flag_gems.vendor_name)` 查看当前使用的后端。
    - Q: 手动设置后端时，需要使用什么格式的供应商名称？
      A: 需要使用 FlagGems 内部识别的供应商名称，通常是小写的硬件平台名称，如 "nvidia"、"amd" 等。
    - Q: 手动设置错误的后端会怎样？
      A: 可能会导致 FlagGems 无法正常工作，或者使用性能较差的后端实现。
    - Q: 如何获取所有可用的后端列表？
      A: 可以查看 FlagGems 的源代码或文档，了解支持的后端列表。

## 与流行框架的集成
- **Hugging Face Transformers**：
  - 何时使用：当你使用 Hugging Face 模型时
  - 集成方式：在推理前启用 FlagGems
  - **FAQ**：
    - Q: 如何在 Hugging Face 模型训练中使用 FlagGems？
      A: 与推理类似，在训练代码前启用 FlagGems 即可，`flag_gems.enable()`。
    - Q: FlagGems 支持哪些 Hugging Face 模型？
      A: 理论上支持所有使用 PyTorch 的 Hugging Face 模型，特别是那些使用了 FlagGems 优化算子的模型，如 Llama、BERT 等。
    - Q: 在 Hugging Face 模型中，哪些算子最有可能从 FlagGems 中受益？
      A: 注意力机制、矩阵乘法、归一化等计算密集型算子最有可能从 FlagGems 中受益。
    - Q: 如何在 Hugging Face 流水线中使用 FlagGems？
      A: 可以在创建流水线前启用 FlagGems，这样整个流水线中的兼容算子都会被加速。
    - Q: FlagGems 与 Hugging Face 的 `accelerate` 库兼容吗？
      A: 是的，FlagGems 与 `accelerate` 库兼容，可以同时使用以获得更好的性能。

- **vLLM**：
  - 何时使用：当你使用 vLLM 进行高吞吐量推理时
  - 集成方式：
    1. 加速标准 PyTorch 算子：`flag_gems.enable()`
    2. 替换 vLLM 特定算子：`flag_gems.apply_gems_patches_to_vllm(verbose=True)`
  - **FAQ**：
    - Q: 为什么需要替换 vLLM 特定算子？
      A: vLLM 有一些自定义的算子实现，替换它们可以获得更好的性能。
    - Q: 替换后会影响 vLLM 的功能吗？
      A: 不会，FlagGems 的实现保持了与原始算子相同的功能，只是性能更好。
    - Q: 如何确定 vLLM 特定算子是否被成功替换？
      A: 可以设置 `verbose=True`，查看控制台输出的替换信息。
    - Q: vLLM 的哪些算子会被 FlagGems 替换？
      A: 通常包括 RMSNorm、RotaryEmbedding、SiluAndMul 等 vLLM 自定义算子。
    - Q: FlagGems 与 vLLM 的版本兼容性如何？
      A: 不同版本的 vLLM 可能有不同的算子实现，建议使用经过验证的 vLLM 版本，或者在新版本中测试兼容性。

## 性能优化
### 预调优模型形状
- **预调优模型形状**：
  - 作用：减少运行时自动调优开销
  - 何时使用：在生产环境中，特别是对延迟敏感的场景
  - 使用方法：运行 `python examples/pretune.py`
  - **FAQ**：
    - Q: 什么是自动调优开销？
      A: Triton 会在第一次运行新形状的算子时尝试不同的配置以找到最佳性能，这个过程会增加延迟。
    - Q: 预调优需要多久？
      A: 取决于你的模型形状数量和复杂度，通常需要几分钟到几十分钟。
    - Q: 预调优结果会保存多久？
      A: 预调优结果会保存在持久缓存中，除非你手动清除缓存，否则会一直保留。
    - Q: 如何确定需要预调优的模型形状？
      A: 应该包含生产环境中常见的输入形状，特别是那些会导致算子形状变化的维度。
    - Q: 预调优后，当输入形状发生变化时会怎样？
      A: 如果输入形状不在预调优缓存中，Triton 会再次进行自动调优，可能会导致延迟峰值。
    - Q: 如何查看预调优缓存的内容？
      A: 预调优缓存通常存储在本地文件系统中，具体位置取决于 Triton 的配置。

### 使用 C++ 包装器
- **使用 C++ 包装器**：
  - 作用：减少 Python 开销，提高性能
  - 何时使用：在延迟敏感或高吞吐量场景中
  - 使用方法：按照 installation 指南安装 C++ 版本
  - **FAQ**：
    - Q: C++ 包装器比 Python 版本快多少？
      A: 取决于具体的算子和使用场景，在某些情况下可以提高 10-20% 的性能。
    - Q: 安装 C++ 版本需要特殊的编译环境吗？
      A: 是的，需要 C++ 编译器和相关依赖，具体要求可以查看 installation 指南。
    - Q: 如何确认我是否使用了 C++ 包装器？
      A: 可以尝试导入 `from flag_gems import c_operators`，如果成功导入，则 C++ 包装器可用。
    - Q: C++ 包装器与 Python 版本的 API 是否兼容？
      A: 是的，C++ 包装器提供与 Python 版本相同的 API，使用方式完全一致。
    - Q: 什么时候应该使用 C++ 包装器？
      A: 在对延迟要求较高的场景，如实时推理服务，或者需要处理大量小批量请求的高吞吐量场景。
    - Q: 使用 C++ 包装器会增加内存使用吗？
      A: 可能会有轻微增加，但通常影响很小，与性能提升相比是值得的。

## 性能加速效果
- **性能对比**：展示 FlagGems 在不同模型和硬件上的加速效果
  - 何时使用：当你需要了解 FlagGems 的性能提升时
  - **FAQ**：
    - Q: 性能加速效果与模型大小有关吗？
      A: 是的，通常模型越大，FlagGems 的加速效果越明显，因为大模型更多地受到计算瓶颈的限制。
    - Q: 如何重现性能测试结果？
      A: 可以使用 FlagGems 提供的基准测试脚本，确保使用相同的模型、硬件和测试方法。
    - Q: 性能加速效果在不同硬件平台上有差异吗？
      A: 是的，不同硬件平台的加速效果可能会有所不同，这取决于硬件特性和 FlagGems 对该平台的优化程度。
    - Q: 除了模型大小，还有哪些因素会影响性能加速效果？
      A: 输入形状、批量大小、算子类型、硬件内存带宽等因素都会影响性能加速效果。
    - Q: 如何测量我的具体应用中 FlagGems 的性能提升？
      A: 可以使用基准测试工具，比较启用和不启用 FlagGems 时的执行时间。

## 常见问题和解决方案
- **硬件平台支持**：支持多种 AI 芯片
  - **FAQ**：
    - Q: FlagGems 支持哪些硬件平台？
      A: 支持 NVIDIA GPU、AMD GPU 以及多种国产 AI 芯片，具体列表可以查看 [Supported Platforms](./docs/features.md#platforms-supported)。
    - Q: 在新的硬件平台上使用 FlagGems 需要注意什么？
      A: 需要确保该平台有对应的 PyTorch 和 Triton 编译器支持，可能需要安装特定的后端依赖。
    - Q: FlagGems 对不同 NVIDIA GPU 架构的支持情况如何？
      A: FlagGems 支持从 Turing 架构开始的 NVIDIA GPU，包括 Ampere、Hopper 等最新架构，不同架构的性能优化可能有所差异。
    - Q: 在国产 AI 芯片上使用 FlagGems 需要特殊配置吗？
      A: 通常需要使用芯片厂商提供的 PyTorch 和 Triton 编译器，并可能需要设置特定的环境变量来指定后端。
    - Q: FlagGems 是否支持在 CPU 上运行？
      A: 目前 FlagGems 主要针对 GPU 优化，CPU 支持有限，建议在 GPU 环境中使用以获得最佳性能。

- **多 GPU 环境使用**：单节点直接启用，多节点每个进程单独初始化
  - **FAQ**：
    - Q: 在多 GPU 环境中，是否需要在每个 GPU 上单独启用 FlagGems？
      A: 不需要，在单节点多 GPU 环境中，只需要启用一次，FlagGems 会自动处理多 GPU 情况。
    - Q: 在多节点环境中，如何确保所有节点都启用了 FlagGems？
      A: 需要在每个节点的每个进程中单独初始化 FlagGems，因为分布式环境中的每个进程都是独立的。
    - Q: 在多 GPU 环境中，FlagGems 如何分配工作负载？
      A: FlagGems 本身不负责工作负载分配，而是依赖于 PyTorch 的分布式机制，如 DataParallel 或 DistributedDataParallel。
    - Q: 在多 GPU 环境中，预调优缓存是否会在 GPU 之间共享？
      A: 通常不会，每个 GPU 有自己的预调优缓存，因为不同 GPU 可能有不同的最佳配置。
    - Q: 在多 GPU 环境中使用 FlagGems 时，内存使用会增加吗？
      A: 可能会有轻微增加，因为每个 GPU 都需要加载 FlagGems 内核和缓存，但通常影响很小。

- **算子不兼容问题**：使用 `unused` 参数禁用特定算子
  - **FAQ**：
    - Q: 如何识别哪些算子有兼容性问题？
      A: 可以通过启用调试日志，观察哪些算子在运行时出现错误，或者性能明显下降。
    - Q: 禁用不兼容的算子后，会影响整体性能吗？
      A: 可能会有轻微影响，但通常比出现错误或严重性能下降要好，你可以只禁用确实有问题的算子。
    - Q: 算子兼容性问题通常表现为什么症状？
      A: 常见症状包括运行时错误、数值结果不一致、性能明显下降等。
    - Q: 如何报告算子兼容性问题？
      A: 可以在 GitHub 仓库中提交 Issue，详细描述问题现象、复现步骤和环境信息。
    - Q: 除了禁用不兼容的算子，还有其他解决兼容性问题的方法吗？
      A: 可以尝试更新 FlagGems 到最新版本，或者检查是否有相关的已知问题和解决方案。

- **验证加速效果**：使用 `record` 参数记录算子调用
  - **FAQ**：
    - Q: 除了查看日志，还有其他方法验证加速效果吗？
      A: 可以使用性能分析工具（如 PyTorch Profiler）比较启用和不启用 FlagGems 时的性能差异。
    - Q: 如何确定 FlagGems 是否真的加速了我的模型？
      A: 可以通过测量模型的推理时间或训练速度，比较启用和不启用 FlagGems 时的差异。
    - Q: 验证加速效果时，需要注意哪些因素？
      A: 需要确保测试环境一致，包括硬件、软件版本、输入数据、批量大小等，以获得准确的比较结果。
    - Q: 如何进行公平的性能比较？
      A: 建议进行多次测试并取平均值，排除系统负载等外部因素的影响。
    - Q: 在不同的硬件平台上，如何比较 FlagGems 的加速效果？
      A: 可以计算加速比（使用 FlagGems 的时间 / 不使用 FlagGems 的时间），这样可以在不同平台之间进行相对比较。

- **与 vLLM 集成**：使用 `enable()` 和 `apply_gems_patches_to_vllm()`
  - **FAQ**：
    - Q: 在 vLLM 中使用 FlagGems 会影响其自身的优化吗？
      A: 不会，FlagGems 会与 vLLM 的优化协同工作，进一步提高性能。
    - Q: 如何在 vLLM 服务器中集成 FlagGems？
      A: 需要在 vLLM 服务器启动时，在 worker 进程中启用 FlagGems，具体方法可以参考 vLLM 集成文档。
    - Q: 在 vLLM 中使用 FlagGems 时，内存使用会增加吗？
      A: 可能会有轻微增加，因为 FlagGems 需要加载额外的内核和缓存，但通常影响很小。
    - Q: 如何在 vLLM 中验证 FlagGems 是否成功集成？
      A: 可以通过查看控制台输出的替换信息，或者比较启用和不启用 FlagGems 时的性能差异。
    - Q: FlagGems 与 vLLM 的哪些版本兼容？
      A: 建议使用较新版本的 vLLM，具体兼容版本可以查看 FlagGems 的文档或 GitHub 仓库。
